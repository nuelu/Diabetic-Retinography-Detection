{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Importing libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data Ingestion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all .npz files from the directory\n",
    "def load_data(directory):\n",
    "    images, labels, genders = [], [], []\n",
    "    npz_files= glob.glob(os.path.join(directory, '*.npz'))\n",
    "    for file in npz_files:\n",
    "        data= np.load(file)\n",
    "        images.append(data['slo_fundus'])\n",
    "        labels.append(data['dr_class'])\n",
    "        genders.append(data['male'])\n",
    "    return np.array(images), np.array(labels), np.array(genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to the datasets\n",
    "train_data_dir= \"C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\"\n",
    "test_data_dir= \"C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/test\"\n",
    "val_data_dir= \"C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the datasets\n",
    "train_images, train_labels, train_genders= load_data(train_data_dir)\n",
    "test_images, test_labels, test_genders= load_data(test_data_dir)\n",
    "val_images, val_labels, val_genders= load_data(val_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train images:  (4476, 200, 200, 3)\n",
      "Shape of test images:  (1914, 200, 200, 3)\n",
      "Shape of validation images:  (641, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "#checking the shape of the datasets\n",
    "print(\"Shape of train images: \", train_images.shape)\n",
    "print(\"Shape of test images: \", test_images.shape)\n",
    "print(\"Shape of validation images: \", val_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing images\n",
    "train_images= train_images/255.0\n",
    "test_images= test_images/255.0\n",
    "val_images= val_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\\1005_right.npz\n",
      "Diabetic Retinopathy Class ('dr_class') value: 0\n",
      "Gender ('male') value: 0\n",
      "file:C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\\1006_left.npz\n",
      "Diabetic Retinopathy Class ('dr_class') value: 0\n",
      "Gender ('male') value: 1\n",
      "file:C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\\1006_right.npz\n",
      "Diabetic Retinopathy Class ('dr_class') value: 0\n",
      "Gender ('male') value: 1\n",
      "file:C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\\100_left.npz\n",
      "Diabetic Retinopathy Class ('dr_class') value: 0\n",
      "Gender ('male') value: 1\n",
      "file:C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\\100_right.npz\n",
      "Diabetic Retinopathy Class ('dr_class') value: 0\n",
      "Gender ('male') value: 1\n"
     ]
    }
   ],
   "source": [
    "#checking for class imbalance\n",
    "data_dir= \"C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\"\n",
    "#getting paths to the .npz files\n",
    "npz_files= glob.glob(os.path.join(data_dir, '*.npz'))\n",
    "#checking values for a few sample files\n",
    "sample_files= npz_files[:5]\n",
    "for file in sample_files:\n",
    "    data= np.load(file)\n",
    "    print(f\"file:{file}\")\n",
    "    print(\"Diabetic Retinopathy Class ('dr_class') value:\", data['dr_class'])\n",
    "    print(\"Gender ('male') value:\", data['male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for 'male' (gender): Counter({1: 2390, 0: 2086})\n",
      "Counts for 'dr_class' (label): Counter({0: 3358, 1: 1118})\n"
     ]
    }
   ],
   "source": [
    "#initializing counters for 'male' and 'dr_class'\n",
    "gender_counter= Counter()\n",
    "label_counter= Counter()\n",
    "\n",
    "#looping through all .npz to count for occurences of values\n",
    "for file in npz_files:\n",
    "    data= np.load(file)\n",
    "    gender_counter[int(data['male'])] +=1\n",
    "    label_counter[int(data['dr_class'])] +=1\n",
    "\n",
    "#print counts for genders and labels\n",
    "print(\"Counts for 'male' (gender):\", gender_counter)\n",
    "print(\"Counts for 'dr_class' (label):\", label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.6664681357951161, 1: 2.001788908765653}\n"
     ]
    }
   ],
   "source": [
    "#calculating class weights to handle imbalance\n",
    "class_weights= class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weight_dict= {0: class_weights[0], 1: class_weights[1]}\n",
    "print(\"Class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 5s/step - AUC: 0.5354 - loss: 1.2067 - val_AUC: 0.4978 - val_loss: 158.3888\n",
      "Epoch 2/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 5s/step - AUC: 0.5009 - loss: 1.1780 - val_AUC: 0.5166 - val_loss: 0.7089\n",
      "Epoch 3/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 5s/step - AUC: 0.5210 - loss: 0.9815 - val_AUC: 0.5000 - val_loss: 0.7011\n",
      "Epoch 4/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 5s/step - AUC: 0.5177 - loss: 0.8693 - val_AUC: 0.5779 - val_loss: 1.2307\n",
      "Epoch 5/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m699s\u001b[0m 5s/step - AUC: 0.5043 - loss: 0.9096 - val_AUC: 0.5643 - val_loss: 0.6983\n",
      "Epoch 6/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 5s/step - AUC: 0.5111 - loss: 0.9150 - val_AUC: 0.5000 - val_loss: 4022.0129\n",
      "Epoch 7/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 5s/step - AUC: 0.5013 - loss: 0.9193 - val_AUC: 0.5604 - val_loss: 5.8407\n",
      "Epoch 8/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 5s/step - AUC: 0.4903 - loss: 0.8703 - val_AUC: 0.4627 - val_loss: 0.7389\n",
      "Epoch 9/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 5s/step - AUC: 0.4812 - loss: 0.8054 - val_AUC: 0.5497 - val_loss: 1.4634\n",
      "Epoch 10/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 5s/step - AUC: 0.4921 - loss: 0.9560 - val_AUC: 0.4809 - val_loss: 1.8470\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - AUC: 0.4410 - loss: 1.0722\n",
      "Test AUC: 0.46387648582458496\n"
     ]
    }
   ],
   "source": [
    "#model architecture (ResNet50)\n",
    "def build_model():\n",
    "    base_model= ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "    model= Sequential(\n",
    "        [base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "#compiling and training the model\n",
    "model= build_model()\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "history= model.fit(\n",
    "    train_images, train_labels, \n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=10, \n",
    "    batch_size=32, \n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "#evaluating the model\n",
    "results= model.evaluate(test_images, test_labels)\n",
    "print(\"Test AUC:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 1s/step\n",
      "Overall AUC: 0.4703142914469158\n",
      "Female AUC: 0.4237088000218414\n",
      "Male AUC: 0.5105547661251337\n"
     ]
    }
   ],
   "source": [
    "#calculating AUC using sklearn for comparison\n",
    "test_predictions = model.predict(test_images).ravel()\n",
    "\n",
    "#overall AUC score\n",
    "overall_auc= roc_auc_score(test_labels, test_predictions)\n",
    "print(\"Overall AUC:\", overall_auc)\n",
    "\n",
    "#separating AUC for Male and Female groups\n",
    "female_indices= np.where(test_genders ==0)[0] #assuming 0= female, 1= male\n",
    "male_indices= np.where(test_genders == 1)[0]\n",
    "\n",
    "female_auc= roc_auc_score(test_labels[female_indices], test_predictions[female_indices])\n",
    "male_auc= roc_auc_score(test_labels[male_indices], test_predictions[male_indices])\n",
    "\n",
    "print(\"Female AUC:\", female_auc)\n",
    "print(\"Male AUC:\", male_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 1s/step - AUC: 0.7210 - loss: 0.6201 - val_AUC: 0.5869 - val_loss: 1.4616\n",
      "Epoch 2/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 1s/step - AUC: 0.8717 - loss: 0.4477 - val_AUC: 0.6961 - val_loss: 0.6140\n",
      "Epoch 3/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 1s/step - AUC: 0.9086 - loss: 0.3796 - val_AUC: 0.7484 - val_loss: 0.6242\n",
      "Epoch 4/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 1s/step - AUC: 0.9479 - loss: 0.2920 - val_AUC: 0.7285 - val_loss: 0.7764\n",
      "Epoch 5/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 1s/step - AUC: 0.9690 - loss: 0.2253 - val_AUC: 0.6674 - val_loss: 0.9649\n",
      "Epoch 6/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m570s\u001b[0m 1s/step - AUC: 0.9815 - loss: 0.1685 - val_AUC: 0.6972 - val_loss: 1.1821\n",
      "Epoch 7/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 1s/step - AUC: 0.9859 - loss: 0.1497 - val_AUC: 0.7162 - val_loss: 0.6977\n",
      "Epoch 8/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 1s/step - AUC: 0.9926 - loss: 0.1067 - val_AUC: 0.6844 - val_loss: 1.4679\n",
      "Epoch 9/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 1s/step - AUC: 0.9930 - loss: 0.1040 - val_AUC: 0.7138 - val_loss: 1.0937\n",
      "Epoch 10/10\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 1s/step - AUC: 0.9928 - loss: 0.0982 - val_AUC: 0.6850 - val_loss: 1.4424\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 437ms/step - AUC: 0.6909 - loss: 0.7173\n",
      "Test AUC: 0.6823305487632751\n"
     ]
    }
   ],
   "source": [
    "#loading and preprocessing the data\n",
    "def preprocess_data(images, labels):\n",
    "    images= images/255.0\n",
    "    return images, labels\n",
    "\n",
    "#synthetic oversampling using SMOTE\n",
    "def apply_smote(images, labels):\n",
    "    #reshape images to 2D for SMOTE\n",
    "    n_samples, height, width, channels= images.shape\n",
    "    flat_images= images.reshape(n_samples, -1)\n",
    "\n",
    "    #applying SMOTE\n",
    "    smote= SMOTE(random_state=42)\n",
    "    oversampled_images, oversampled_labels= smote.fit_resample(flat_images, labels)\n",
    "    \n",
    "    #reshaping images back to 3D\n",
    "    oversampled_images= oversampled_images.reshape(-1, height, width, channels)\n",
    "    return oversampled_images, oversampled_labels\n",
    "\n",
    "#building EfficientNetB0 model\n",
    "def build_efficientnet_model():\n",
    "    base_model= EfficientNetB0(weights='imagenet', include_top=False, input_shape=(200, 200, 3))\n",
    "    model= Sequential(\n",
    "        [base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "#loading the datasets\n",
    "train_images, train_labels, train_genders = load_data(\"C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/train\")\n",
    "val_images, val_labels, val_genders = load_data(\"C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/val\")\n",
    "test_images, test_labels, test_genders = load_data(\"C:/Users/eutomi/Downloads/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos-20241202T032045Z-001/Problem_1_Diabetic_Retinopathy_Detection_using_Color_Fundus_Photos/ODIR_Data/test\")\n",
    "\n",
    "#preprocessing the data\n",
    "train_images, train_labels= preprocess_data(train_images, train_labels)\n",
    "val_images, val_labels= preprocess_data(val_images, val_labels)\n",
    "test_images, test_labels= preprocess_data(test_images, test_labels)\n",
    "\n",
    "#applying SMOTE to balance the training data\n",
    "oversampled_images, oversampled_labels = apply_smote(train_images, train_labels)\n",
    "\n",
    "#building and compiling the model\n",
    "model= build_efficientnet_model()\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "#training the model using the validation set\n",
    "history= model.fit(\n",
    "    oversampled_images, oversampled_labels,\n",
    "    validation_data=(val_images, val_labels),\n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "#evluating the model on the test set\n",
    "results= model.evaluate(test_images, test_labels)\n",
    "print(\"Test AUC:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 482ms/step\n",
      "Overall AUC: 0.7071679373996789\n",
      "Female AUC: 0.6729051061012484\n",
      "Male AUC: 0.7361747945446289\n"
     ]
    }
   ],
   "source": [
    "#metrics for the model\n",
    "test_predictions= model.predict(test_images).ravel()\n",
    "\n",
    "#overall AUC score\n",
    "overall_auc= roc_auc_score(test_labels, test_predictions)\n",
    "print(\"Overall AUC:\", overall_auc)\n",
    "\n",
    "#gender-based AUC scores\n",
    "female_indices= np.where(test_genders == 0)[0]\n",
    "male_indices= np.where(test_genders == 1)[0]\n",
    "\n",
    "female_auc= roc_auc_score(test_labels[female_indices], test_predictions[female_indices])\n",
    "male_auc= roc_auc_score(test_labels[male_indices], test_predictions[male_indices])\n",
    "\n",
    "print(\"Female AUC:\", female_auc)\n",
    "print(\"Male AUC:\", male_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
